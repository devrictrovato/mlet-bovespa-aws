# üìà mlet-bovespa-aws

> Pipeline para scraping e processamento de dados do preg√£o da B3, utilizando apenas os arquivos presentes no reposit√≥rio.

## üóÇ Estrutura do reposit√≥rio

```bash
mlet-bovespa-aws/
‚îú‚îÄ‚îÄ scraping/ # Script(s) para coletar dados da B3 (web scraping)
‚îú‚îÄ‚îÄ glue/ # Projeto visual do job de ETL (visual, formato exportado)
‚îú‚îÄ‚îÄ lambda/ # C√≥digo da fun√ß√£o local que iniciaria o job (exemplo)
‚îú‚îÄ‚îÄ s3/ # Estrutura de pastas esperada: raw/, refined/
‚îî‚îÄ‚îÄ README.md # Este arquivo
```


## üîß Funcionalidades sem AWS

- O diret√≥rio `scraping/` cont√©m script(s) que fazem o download dos dados do preg√£o da B3.
- Dentro de `glue/`, h√° o job de transforma√ß√£o com l√≥gica visual (agrupamento, renomea√ß√£o e c√°lculo de datas).
- A pasta `lambda/` mostra um exemplo de fun√ß√£o que acionaria esse job.
- A pasta `s3/` ilustra como os dados seriam organizados localmente com as pastas `raw/` (dados brutos) e `refined/` (dados processados), ambos em formato Parquet, particionados por data e ticker.

## üß™ Como executar localmente

1. Execute o script de scraping para obter arquivos `.parquet` no diret√≥rio `s3/raw/YYYY/MM/DD/`.
2. Simule a l√≥gica visual do Glue com ferramentas como Pandas ou PySpark:
   - Execute agrega√ß√µes e sumariza√ß√µes (ex.: soma de volumes).
   - Renomeie pelo menos duas colunas.
   - Realize um c√°lculo envolvendo campos de data (ex.: diferen√ßa entre data de preg√£o e data de registro).
3. Salve os dados transformados no diret√≥rio `s3/refined/YYYY/MM/DD/`, no formato Parquet.

## ‚úÖ Requisitos contemplados (sem AWS)

| Requisito       | Descri√ß√£o |
|----------------|-----------|
| Scraping       | Extra√ß√£o de dados da B3 via script. |
| Formato Parquet | Armazenamento em Parquet local. |
| Parti√ß√£o por data | Organiza√ß√£o em subpastas por data. |
| Transforma√ß√µes | Agrega√ß√£o, renomea√ß√£o e c√°lculo envolvido datas. |
| Sa√≠da refinada | Arquivos refinados em `s3/refined/...`. |

## ‚ö†Ô∏è Observa√ß√µes

- Este reposit√≥rio exemplifica a l√≥gica do pipeline sem utilizar servi√ßos de nuvem.
- Para uso em produ√ß√£o ou escalabilidade, considere adaptar os scripts para execu√ß√£o em data lakes, Spark, PostgreSQL, etc.
- Os c√≥digos demonstram o fluxo esperado, mas n√£o s√£o auto‚Äëexecut√°veis sem ambiente local configurado, sendo executado sobdemanda.

---
